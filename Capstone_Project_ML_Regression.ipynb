{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anshul-Baghel-03/Supervised-ML-Regression/blob/main/Capstone_Project_ML_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -  Yes Bank Stock Closing Price Prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual\n",
        "##### **Name** -  Anshul Baghel\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes Bank was a very reputed bank till 2018. After 2018, the bank came under the umbrella of risk inflated banks because of the fraud case by Rana kapoor. This project would help not only yes bank but to all those banks who want to predict their future and are in conundrum for their future. So Machine Learning is helping us to resolve the issue of all those companies and firms who want to gather some courage in order to survive in the market for a longer time. By predicting the price with the acquaintance of Machine Learning especially the linear Regression and other regressors, which helped firms and companies to sustain in the market. In this project the monthly Open,Close,Low and High prices of Yes Bank stock have helped to train the model on which learning occurred and then the respective prediction occurs."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes Bank is a well-known bank in the Indian financial domain. Since 2018, it has been in the news because of the fraud case involving Rana Kapoor. Owing to this fact, it was interesting to see how that impacted the stock prices of the company and whether Time Series models or any other predictive models can do justice to such situations. This Dataset has monthly stock prices of the bank since its inception and includes closing, opening, highest and lowest stock prices of every month. The main objective is to predict the closing stock price of the month."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import (Lasso, Ridge,\n",
        "ElasticNet)\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import (r2_score,\n",
        "mean_squared_error,  mean_absolute_percentage_error,\n",
        "mean_absolute_error)\n",
        "from sklearn import metrics\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "## Mounting google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Creating path for the dataset\n",
        "yesbank_data='/content/drive/MyDrive/Colab Notebooks/data_YesBank_StockPrices.csv'\n",
        "yesbank_df=pd.read_csv(yesbank_data)"
      ],
      "metadata": {
        "id": "4Zo9epxMqQLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "yesbank_df.head() ##  shows the top 5 rows"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yesbank_df.tail() ## shows the last 5 rows"
      ],
      "metadata": {
        "id": "iKp411c8rD_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "yesbank_df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "yesbank_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "len(yesbank_df[yesbank_df.duplicated()])"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**There are no duplicate values in the dataset.**\n"
      ],
      "metadata": {
        "id": "W-Ghmvjhrnuk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "yesbank_df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**There are no null values in the dataset.**"
      ],
      "metadata": {
        "id": "22HZMFhfr416"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "null_counts= yesbank_df.isnull().sum()\n",
        "null_counts.plot.bar()\n",
        "plt.title('Null Value Counts')\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The given dataset of yesbank has 185 rows and 5 Columns/Features in which we have the target label/dependent variable- \"close\"{i.e. we have to predict the closing stock price} with help of our independent variable i.e.- \"Date\", \"open\", \"High\", \"Low\".There are no null and duplicate values in the dataset.\"Date\" feature have object datatype which will be converted in datetime datatype in further process.Answer Here"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "yesbank_df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "## It gives the Statistical Information about the numeric column.\n",
        "yesbank_df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the given dataset there are 185 rows and 5 features,Features description are as follows-\n",
        "\n",
        "*   **Date**-: Date denotes the date of investment(date contains month and year for a particular price.\n",
        "*   **Open**-: It is the price at which a stock started trading.\n",
        "*   **High**-: It is the highest price at which a stock is traded during a period.\n",
        "\n",
        "*   **Low**-: It is the minimum price at which a stock is traded during a period.\n",
        "\n",
        "*   **Close**-: The closing price refers to a stock's trading price closed at the end of a trading day .It's a dependent variable that we need to predict from our respective ML models.The closing price is calculated as the weighted average price of the last 30 minutes, i.e. from 3:00 PM to 3:30 PM in case of equity.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "yesbank_df.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "## As we have seen in dataset that 'DATE' feature is in not the correct format, so we have to change it in 'Datetime' format\n",
        "# First check the format of 'Date' Feature\n",
        "yesbank_df['Date']"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above output it is seen that DATE is in the format of **MMM-YY** but we have to convert it into proper date format like **YYYY-MM-DD**."
      ],
      "metadata": {
        "id": "AUD2rVEGuWjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting Date to Datetime format(YYYY-MM-DD)\n",
        "yesbank_df['Date']= pd.to_datetime(yesbank_df['Date'].apply(lambda x: datetime.strptime(x, '%b-%y')))"
      ],
      "metadata": {
        "id": "DXMrB0NVukf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yesbank_df['Date']"
      ],
      "metadata": {
        "id": "lb1JtovmurNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Again checking the dataset information regarding its datatype\n",
        "yesbank_df.info()"
      ],
      "metadata": {
        "id": "27pzwMhEuy-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But the problem is that our ML models doesn't work on \"Date\" data.So we need to convert it into numerical column.But, numerical date have no use in our respective dataframe to predict the goal .So,make the \"Date\" column as dataframe index ."
      ],
      "metadata": {
        "id": "3S4jiWPMu7CV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Converting 'Date' Feature to dataframe Index.\n",
        "yesbank_df.set_index('Date',inplace=True)"
      ],
      "metadata": {
        "id": "auWCAFLZvEZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Checking the dataframe with index 'Date'\n",
        "yesbank_df.head()"
      ],
      "metadata": {
        "id": "uwlnWkBevHM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The given dataset has 185 rows and 5 columns/features having no null and duplicates values.'Date' Feature is not in proper format so it is converted to 'datetime' format and make it to the Index of the dataframe as per our need to proceed further.Answer Here."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#*Univariate Analysis*\n",
        "\n",
        "#### Chart - 1.Histogram and KDE Plot of Dependent Variable 'Close'"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# Plotting the histogram to see Dependent variable 'Close' distribution which we need to predict later\n",
        "plt.figure(figsize=(7,7))\n",
        "sns.distplot(yesbank_df['Close'],color=\"pink\")\n",
        "plt.title(\"Close stock price distirbution\")"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have picked the above chart because it combines both histogram and kde plot that offers a comprehensive visualization of the data distribution. It allows for a better understanding of the distribution's characteristics, such as its shape, peaks, and deviations from a normal distribution.The combined plot provides a richer visualization that incorporates both the frequency-based information from the histogram and the smooth density estimate from the KDE plot."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above chart it is clearly visualize that it is right/positively skewed and has to be converted to normal distribution."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Currently can't say that it has a positive or negative impact but it is helpful to understand and decide upon the requirement of transformation of the features for Model implementation.Here we will use log transformation to convert it into normal distribution"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2.Log Transformation of Dependent Variable 'Close'"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "#  Plotting the Log Transformation to see Dependent variable 'Close' distribution which we need to predict later.\n",
        "plt.figure(figsize=(7,7))\n",
        "sns.distplot(np.log(yesbank_df['Close']),color=\"r\")\n",
        "plt.title(\"Close Price Distribution after log transformation\")"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used the log transformation because the distribution is not much skewed, and log transformation is helpful to bring the normal pattern in distribution of dependent feature.Beacuse of the Log transformation outliners are removed."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Log transformation is sufficient to bring the noraml distribution.It shows the mean is pumped and the frequent points are not near to mean. The plot clarifies about the bubble price of Yes Bank stock remained for very less time."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It helps to observe the peak and vallyes in closing stock price.The inflated price at mean is temporary as it is a bubble point and after this the price got decline tremendously because of the fraud case which happened in 2018."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3.Histogram,KDE and Box plot of Independent Variables"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "# Initializing variable for indepenedent features.\n",
        "indp_numeric_features = yesbank_df.describe().columns[0:3]\n",
        "indp_numeric_features\n",
        "\n",
        "for col in indp_numeric_features:\n",
        "    plt.figure(figsize=(25, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.distplot(yesbank_df[col], color=\"blue\")\n",
        "    plt.title('Distribution Curve')\n",
        "\n",
        "# The Axes. axvline() function in axes module of matplotlib library is used to add a vertical line across the axis.\n",
        "# It will show where the \"mean\" and \"median\" lie for each plot\n",
        "\n",
        "    plt.ylabel(\"Density\", size=14)\n",
        "    plt.axvline(yesbank_df[col].mean(),color='magenta',linewidth=1.5)\n",
        "    plt.axvline(yesbank_df[col].median(),color='red',linestyle=\"dashed\",linewidth=1.5)\n",
        "\n",
        "# using subplot() function of matplotlib to create boxplot in this figure itself\n",
        "# Box plot is used to check outliers are present in respective features or not\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.title('Box Plot')\n",
        "    sns.boxplot(y=yesbank_df[col], color=\"green\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have picked the above chart because it combines histogram,kde and box plot that offers a comprehensive visualization of the data distribution and outliner as well. It allows for a better understanding of the distribution's characteristics, such as its shape, peaks, and deviations from a normal distribution.The combined plot provides a richer visualization that incorporates both the frequency-based information from the histogram and the smooth density estimate from the KDE plot and in Box plot the quartile divides the data in four equal parts from which we can recognize max.,min.,mean and median of the data."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above chart it is clearly visualize that it is right/positively skewed and has to be converted to normal distribution and by converting it to normal distribution outliners can be removed."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Currently can't say that it has a positive or negative impact but it is helpful to understand and decide upon the requirement of transformation of the features for Model implementation.Here we will use log transformation to convert it into normal distribution and to remove outliner."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4. Log Transformation of Independent Variables\n"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# log tranformation to convert Independent Feautres to normal distribution\n",
        "\n",
        "for col in indp_numeric_features:\n",
        "    plt.figure(figsize=(25, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.title(\"Distribution Curve\")\n",
        "\n",
        "# np.log() is a method in numpy library to convert our dataset values into log transformation to get a normal distribution curve\n",
        "\n",
        "    feature_to_log = np.log(yesbank_df[col])  # assign log tranformation value into a variable\n",
        "    sns.distplot(feature_to_log, color=\"blue\")\n",
        "\n",
        "# The Axes. axvline() function in axes module of matplotlib library is used to add a vertical line across the axis.\n",
        "# It will show where the \"mean\" and \"median\" lie for each plot\n",
        "\n",
        "    plt.ylabel(\"Density\", size=14)\n",
        "    plt.axvline(feature_to_log.mean(),color='magenta',linewidth=1.5)\n",
        "    plt.axvline(feature_to_log.median(),color='red',linestyle=\"dashed\",linewidth=1.5)\n",
        "\n",
        "# creating boxplot to see if there is any outliers in any feature or not\n",
        "# using subplot() function of matplotlib to create boxplot in this figure itself\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.title(\"Box plot\")\n",
        "    sns.boxplot(y=feature_to_log, color=\"green\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used the log transformation because the distribution is not much skewed, and log transformation is helpful to bring the normal pattern in distribution of dependent feature.Beacuse of the Log transformation outliners are removed."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Log transformation is sufficient to bring the noraml distribution.The plot clarifies about the bubble price of Yes Bank stock remained for very less time.we can see from the distribution curve that mean is now closer median.\n",
        "\n",
        "From the above boxplot after log transformation, we can see outliner are removed and we have approximate result of quartiles for independent features which are as follows-\n",
        "\n",
        "*   For feature Open- Lower Quartile(Q1)- 3.6 ,Median(Q2)- 4.3, Upper Quartile(Q3)- 5.0\n",
        "*   For feature High- Lower Quartile(Q1)- 3.7 ,Median(Q2)- 4.4, Upper Quartile(Q3)- 5.2\n",
        "\n",
        "*   For feature Low- Lower Quartile(Q1)- 3.3 ,Median(Q2)- 4, Upper Quartile(Q3)- 4.9\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It helps to observe the peak and vallyes in stock prices.The inflated price at mean is temporary as it is a bubble point and after this the price got decline tremendously because of the fraud case which happened in 2018.\n",
        "\n",
        "After the log transformation, the outliner are removed and the distribution is converted to normal pattern which will suffice the model requirements and help to achieve better accuracy of our models,so we can say that the transformation has a positive impact."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#*Bivariate Analysis*\n",
        "#### Chart - 5.Lineplot"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Visualizing yesbank stock closing price over the time.\n",
        "sns.set_theme(style=\"white\")\n",
        "sns.set(rc={'figure.figsize':(11,8)})\n",
        "sns.lineplot(x=\"Date\", y=\"Close\",data=yesbank_df,color='r').set(title='Yes Bank closing price',xlabel='Year')\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line plot is used to show the progression of a variable over time or any continuous variable that has an inherent order. They are particularly useful for visualizing trends, seasonality, and changes in values over time."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above plot we can easily see that price has increasing trend from 2009-2108 but after 2018 there is sudden drop in the prices because of the fraud case involving Rana Kapoor."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we know the stock closing price serves as a benchmark for determining how a stock performs and also help investors comprehend how its value has changed over time.From the above plot it is seen that stock closing price diminishes continously after 2018 ,so it is alarming for the yes bank to cope with this situation as closing price is that one price which drives investors to invest in a stock or not.So, as per scnerio the insights have a negative impact on business, but by observing the trend they must try to restrict the stock manipulation to become stable in terms of revenue."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6. Scatter Plot(Checking Relationship b/w Independent Variable and Dependent Variable)"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "# Visualizing the relation b/w independent and dependent variable\n",
        "plt.figure(figsize = (15,6))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.scatter(yesbank_df['Close'], yesbank_df['Open'], alpha=0.6)\n",
        "plt.title('Open and Close ')\n",
        "plt.xlabel('Open')\n",
        "plt.ylabel('Class')\n",
        "sns.regplot(x ='Open', y = 'Close', data= yesbank_df)\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.scatter(yesbank_df['Close'], yesbank_df['High'], alpha=0.6)\n",
        "plt.title('High and Close ')\n",
        "plt.xlabel('High')\n",
        "plt.ylabel('Class')\n",
        "sns.regplot(x ='High', y = 'Close', data= yesbank_df)\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.scatter(yesbank_df['Close'], yesbank_df['Low'], alpha=0.6)\n",
        "plt.title('Low and Close ')\n",
        "plt.xlabel('Low')\n",
        "plt.ylabel('Class')\n",
        "sns.regplot(x ='Low', y = 'Close', data= yesbank_df)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scatter plot is used because it is easy to visualize the relationship between two variables.They help you understand if there is any correlation or pattern between the variables.Scatter plots are useful for assessing the strength and direction of the relationship between two variables. By examining the overall pattern of the data points, we can determine if there is a positive correlation (both variables increase together), negative correlation (one variable increases while the other decreases), or no correlation between the variables."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above plot it is visible that dependent variable is highly related to all independent variables which is a good sign while implementing the models."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights have a positive correlation between independent and dependent variables which is a plus for our models but we have to be cautious about the multicollinearity and try to merge the features or drop them to rectify the multicollinearity which will ultimately increases the model accuracy."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7.Heatmap(To check the Correlation b/w all features)"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "## First step to check multicollinearity\n",
        "plt.figure(figsize=(10,6))\n",
        "correlation = yesbank_df.corr()\n",
        "sns.heatmap(abs(correlation), annot = True, cmap='YlGnBu')"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Heatmaps are used to visually represent data in a matrix form where each cell's color is determined by its value. Heatmaps are commonly used in data visualization to represent patterns or relationships in data, and to identify areas of high and low values or activity.Heatmap consists of small square boxes having a numeric value known as correlation coefficient.A correlation coefficient is a statistical measure that indicates the strength and direction of a linear relationship between two variables. A correlation coefficient of 0 indicates no linear relationship between the two variables, while a correlation coefficient of 1 indicates a perfect positive linear relationship (i.e., as one variable increases, the other variable increases by a constant proportion). A correlation coefficient of -1 indicates a perfect negative linear relationship (i.e., as one variable increases, the other variable decreases by a constant proportion)."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above plot,it is very significant that there is high collinearity.Each and every feature is correlated with every other feature.(i.e. Multicollinearity)"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights are useful to understand and get an idea about the collinearity of the features so that we could deal with it in future model implentation."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# Checking for missing values in the dataset.\n",
        "yesbank_df.isnull().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can with the help of above code, there is no missing values in the dataset."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "yesbank_df1=yesbank_df.iloc[:,0:].copy() # Copying will secure the main dataframe\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yesbank_df1.head()"
      ],
      "metadata": {
        "id": "KMEvPSdUhLb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace outliers with null values\n",
        "Q1 = yesbank_df1.quantile(0.25)\n",
        "Q3 = yesbank_df1.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "yesbank_df1[(yesbank_df1 < lower_bound) | (yesbank_df1 > upper_bound)] = np.nan\n",
        "\n",
        "# Replace null values with median\n",
        "median = yesbank_df1.median()\n",
        "yesbank_df1 = yesbank_df1.fillna(median)"
      ],
      "metadata": {
        "id": "IlYDbztyhMC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yesbank_df1.isnull().sum() # Crossverifying the null values,that it is replaced or not."
      ],
      "metadata": {
        "id": "g0fvjb4Oi9qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly I went with log transformation of features in the visualization part where transformation to the data can sometimes help to normalize the distribution and reduce the impact of outliers.This transformation can make the data more suitable for analysis while minimizing the effect of extreme values but it is not the most robust method to deal with outliers.\n",
        "\n",
        "Secondly, I came up with Interquartile Range (IQR) method which is a robust statistical method in which I replaced the outliers of all feature with its respective median so that the distribution do not drastically change and should not be affected by the outliers."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generally it might happen when we replaced outliers with median, our correlation b/w independent and dependent variable got distorted that will affect our model accuracy, so analyzing again the correlation b/w independent and variable, if distorted then will apply log transformation to data frame (yesbank_df1) which is the copy of main data frame."
      ],
      "metadata": {
        "id": "ZxZpi2ksjdQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "# Manipulating the feature/variable-\n",
        "corr = yesbank_df1.corr()\n",
        "cmap = cmap=sns.diverging_palette(5, 250, as_cmap=True)\n",
        "\n",
        "def magnify():\n",
        "    return [dict(selector=\"th\",\n",
        "                 props=[(\"font-size\", \"7pt\")]),\n",
        "            dict(selector=\"td\",\n",
        "                 props=[('padding', \"0em 0em\")]),\n",
        "            dict(selector=\"th:hover\",\n",
        "                 props=[(\"font-size\", \"12pt\")]),\n",
        "            dict(selector=\"tr:hover td:hover\",\n",
        "                 props=[('max-width', '200px'),\n",
        "                        ('font-size', '12pt')])\n",
        "]\n",
        "\n",
        "corr.style.background_gradient(cmap, axis=1)\\\n",
        "    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\\\n",
        "    .set_caption(\"Hover to magify\")\\\n",
        "    .set_table_styles(magnify())"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "## Feature selection for Independent and dependent variable\n",
        "independent_variable= [i for i in yesbank_df1.columns if i not in ['Close']]\n",
        "dependent_variable= 'Close'\n"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "'Open', 'High', 'Low' are Important features as they contain maximum information which will be very helpful for prediction."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "yesbank_df1['Open']= np.log(yesbank_df1['Open'])\n",
        "yesbank_df1['High']= np.log(yesbank_df1['High'])\n",
        "yesbank_df1['Low']= np.log(yesbank_df1['Low'])\n",
        "yesbank_df1['Close']= np.log(yesbank_df1['Close'])"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yesbank_df1.head()"
      ],
      "metadata": {
        "id": "K_YjS3qRlLK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Checking the correlation b/w independent and dependent variable that it is restored or not, after data transformation.\n",
        "cor_log=yesbank_df1.corr()\n",
        "sns.heatmap(cor_log,annot=True)\n"
      ],
      "metadata": {
        "id": "dzDWzavElb6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Transformation is required and I have used log transformation(np.log) for the data which is given. The reasons behind the transformation are as follows-\n",
        "\n",
        "\n",
        "*   Data which is given is right skewed so to make the distribution normal and symmetric transforamtion is required which will help to implement model correctly and pricesly which help us to reach towards desired accuracy.\n",
        "*   After treating outliers the correltion b/w independent and dependent variable got distorted so to restore that, transformation is required and as we can see from the above heatmap the correlation are restored which will make our model more accurate for prediction, but it has multicollinearity and to deal with it we have to drop that feature which is least correlated with the target variable, but by doing so we lose the valuable information as our dataset is small, so we continue with the multicollinearity and check how our model behaves with this phenomena.\n",
        "\n"
      ],
      "metadata": {
        "id": "L195fUv2tkL4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "X= yesbank_df1[independent_variable].values\n",
        "y=yesbank_df1[dependent_variable].values\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,Y_train,Y_test = train_test_split( X ,y , test_size= 0.2,random_state=0)\n",
        "print(X_train.shape,X_test.shape)\n",
        "print(Y_train.shape , Y_test.shape)"
      ],
      "metadata": {
        "id": "TUBvJFqOlvMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the Data spliting, the 20% data has been splitted as test data. As the data is already very small 20 % would be enough to test upon the training model.\n"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Scaling"
      ],
      "metadata": {
        "id": "4Ka5B2g3smf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "scaler= MinMaxScaler()\n",
        "X_train1= scaler.fit_transform(X_train)\n",
        "X_test1 = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "7vimN-IbssTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "olr52Gv8s1ml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Standard and MinMax Scaler to scale the data.The reason to use scaling is that it provides benefits in terms of algorithm performance, convergence speed, and interpretability."
      ],
      "metadata": {
        "id": "Ahjph_XYsth1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "reg  = LinearRegression()\n",
        "# Fit the Algorithm\n",
        "reg.fit(X_train1 , Y_train)\n",
        "# Predict on the model\n",
        "y_pred = reg.predict(X_test1)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear-regression models are relatively simple and provide an easy-to-interpret mathematical formula that can generate predictions. Linear regression can be applied to various areas in business and academic study.\n",
        "\n",
        "*   well find that linear regression is used in everything from biological, behavioral,financial and stocks, environmental and social sciences to business. Linear-regression models have become a proven way to scientifically and reliably predict the future. Because linear regression is a long-established statistical procedure, the properties of linear-regression models are well understood and can be trained very quickly.\n",
        "*   The linear regression model returns an equation that determines the relationship between the independent variables and the dependent variable.\n",
        "\n"
      ],
      "metadata": {
        "id": "lesiPMHDmq3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "train_accuracy = reg.score(X_train1,Y_train)\n",
        "test_accuracy = r2_score(Y_test,y_pred)\n",
        "print( 'Train accuracy is ',train_accuracy)\n",
        "print(' Test accuracy is ', test_accuracy)\n",
        "print(\"MSE value :\", round(mean_squared_error(Y_test, y_pred), 4))\n",
        "print(\"RMSE value :\", round(math.sqrt(mean_squared_error(Y_test, y_pred)), 4))\n",
        "print(\"MAE value :\", round(mean_absolute_error(Y_test, y_pred), 4))\n",
        "print(\"MAPE value :\", round(mean_absolute_percentage_error(Y_test, y_pred), 4))"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Liner Regression plot- Actual price vs predicted price.\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot((y_pred),color='green')\n",
        "plt.plot((Y_test))\n",
        "plt.suptitle('Actual Vs Predicted Close Price:Linear Regression', fontsize =14)\n",
        "plt.legend([\"Predicted\",\"Actual\"], fontsize=14)\n",
        "plt.xlabel('No of Test Data', fontsize= 14)\n",
        "plt.ylabel('Closing Price', fontsize= 14)\n",
        "plt.grid(True)"
      ],
      "metadata": {
        "id": "zKhHFyOOnja8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation\n",
        "lasso= Lasso(alpha=0.00001)\n",
        "# Fit the Algorithm\n",
        "lasso.fit(X_train1,Y_train)\n",
        "# Predict on the model\n",
        "y_pred_lasso = lasso.predict(X_test1)"
      ],
      "metadata": {
        "id": "5OFN-YqTzqV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LASSO stands for Least Absolute Shrinkage and Selection Operator. I know it doesnt give much of an idea but there are 2 key words here  absolute and selection.\n",
        "\n",
        "Lasso regression performs L1 regularization, i.e. it adds a factor of sum of absolute value of coefficients in the optimization objective. Thus, lasso regression optimizes the following:\n",
        "\n",
        "Objective = RSS +  * (sum of absolute value of coefficients)\n",
        "\n",
        "Here,  (alpha) provides a trade-off between balancing RSS and magnitude of coefficients. can take various values. Lets iterate it here briefly:\n",
        "\n",
        "1. = 0: Same coefficients as simple linear regression.\n",
        "\n",
        "2. = : All coefficients zero (same logic as before).\n",
        "\n",
        "3.0 <  < : coefficients between 0 and that of simple linear regression."
      ],
      "metadata": {
        "id": "qbt4nFEwn1l4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing using evaluation Metric Score chart\n",
        "train_accuracy = lasso.score(X_train1,Y_train)\n",
        "test_accuracy = r2_score(Y_test,y_pred_lasso)\n",
        "print( 'Train accuracy is ',train_accuracy)\n",
        "print(' Test accuracy is ', test_accuracy)\n",
        "print(\"MSE value :\", round(mean_squared_error(Y_test, y_pred_lasso), 4))\n",
        "print(\"RMSE value :\", round(math.sqrt(mean_squared_error(Y_test, y_pred_lasso)), 4))\n",
        "print(\"MAE value :\", round(mean_absolute_error(Y_test, y_pred_lasso), 4))\n",
        "print(\"MAPE value :\", round(mean_absolute_percentage_error(Y_test, y_pred_lasso), 4))"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lasso Regression plot- Actual price vs predicted price.\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot((y_pred_lasso),color='red')\n",
        "plt.plot((Y_test))\n",
        "plt.suptitle('Actual Vs Predicted Close Price:Lasso Regression', fontsize =14)\n",
        "plt.legend([\"Predicted\",\"Actual\"], fontsize=14)\n",
        "plt.xlabel('No of Test Data', fontsize= 14)\n",
        "plt.ylabel('Closing Price', fontsize= 14)\n",
        "plt.grid(True)"
      ],
      "metadata": {
        "id": "isrssT5yoHHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning of Lasso Regularization"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-validation with hyperparameter tuning refers to the process of combining cross-validation with the search for the optimal hyperparameters of a machine learning model. The advantage of using cross-validation with hyperparameter tuning is that it provides a more reliable estimate of the model's performance by evaluating different hyperparameter combinations on multiple folds of the data. It helps to avoid overfitting and ensures that the selected hyperparameters generalize well to unseen data."
      ],
      "metadata": {
        "id": "x0Cu2XoPoy7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV)\n",
        "lasso_cv = Lasso()\n",
        "# Fit the Algorithm\n",
        "parameters = {'alpha': [1e-15,1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1e-1,1,5,10,20,30,40,45,50,55,60,100]}\n",
        "lasso_regressor = GridSearchCV(lasso_cv, parameters, scoring='neg_mean_squared_error', cv=3)\n",
        "lasso_regressor.fit(X_train1, Y_train)\n",
        "print(\"The best fit alpha value is found out to be :\" ,lasso_regressor.best_params_)\n",
        "print(\"\\nUsing \",lasso_regressor.best_params_, \" the negative mean squared error is: \", lasso_regressor.best_score_)"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lasso_best=Lasso(alpha = 0.001)\n",
        "lasso_best.fit(X_train1,Y_train)"
      ],
      "metadata": {
        "id": "0yDon-yFo4O4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the model\n",
        "y_pred_best=lasso_best.predict(X_test1)"
      ],
      "metadata": {
        "id": "izv6oIsBo4ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing using evaluation Metric Score chart\n",
        "train_accuracy = lasso_best.score(X_train1,Y_train)\n",
        "test_accuracy = r2_score(Y_test,y_pred_best)\n",
        "print( 'Train accuracy is ',train_accuracy)\n",
        "print(' Test accuracy is ', test_accuracy)\n",
        "print(\"MSE value :\", round(mean_squared_error(Y_test, y_pred_best), 4))\n",
        "print(\"RMSE value :\", round(math.sqrt(mean_squared_error(Y_test, y_pred_best)), 4))\n",
        "print(\"MAE value :\", round(mean_absolute_error(Y_test, y_pred_best), 4))\n",
        "print(\"MAPE value :\", round(mean_absolute_percentage_error(Y_test, y_pred_best), 4))"
      ],
      "metadata": {
        "id": "5u879RUEpFbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Lasso Regression plot after cross-validation - Actual price vs predicted price.\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot((y_pred_best),color='aqua')\n",
        "plt.plot((Y_test))\n",
        "plt.suptitle('Actual Vs Predicted Close Price:Lasso Regression(cross validation)', fontsize =14)\n",
        "plt.legend([\"Predicted\",\"Actual\"], fontsize=14)\n",
        "plt.xlabel('No of Test Data', fontsize= 14)\n",
        "plt.ylabel('Closing Price', fontsize= 14)\n",
        "plt.grid(True)"
      ],
      "metadata": {
        "id": "f561kvbEpFwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used GridSearch cross validation technique. It gives functionality for parameter evaluating at the given regularizrion technique."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   In the normal Lasso regulariztion the accuracy for train and test is 97% and 94% respectively with RMSE- 0.2211\n",
        "*   But after cross validation with new value of alpha, the train and test accuracy become 96% and 93% respectively with RMSE- 0.2297\n",
        "\n",
        "There is decrease in training as well as testing accuracy after cross-validation,this means model approaches towards slightly generalized fitting.\n"
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "ridge=Ridge()\n",
        "\n",
        "# Fit the Algorithm\n",
        "ridge.fit(X_train1,Y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_ridge= ridge.predict(X_test1)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Ridge regression performs **L2 regularization**, i.e. it adds a factor to sum of squares of coefficients in the optimization objective. Thus, ridge regression optimizes the following:\n",
        "\n",
        " **Objective = RSS +  * (sum of square of coefficients)**\n",
        "\n",
        " Here,  (alpha) is the parameter which balances the amount of emphasis given to minimizing RSS vs minimizing sum of square of coefficients.  can take various values:\n",
        "\n",
        " 1. ** = 0**:\n",
        "\n",
        "\n",
        "* The objective becomes same as simple linear regression.\n",
        "* Well get the same coefficients as simple linear regression.\n",
        "\n",
        "2. ** = **:\n",
        "\n",
        "\n",
        "* The coefficients will be zero because of infinite weightage on square of coefficients, anything less than zero will make the objective infinite.\n",
        "\n",
        "3. **0 <  < **:\n",
        "\n",
        "\n",
        "* The magnitude of  will decide the weightage given to different parts of objective.\n",
        "* The coefficients will be somewhere between 0 and ones for simple linear regression.\n",
        "\n",
        "I hope this gives some sense on how  would impact the magnitude of coefficients. One thing is for sure that any non-zero value would give values less than that of simple linear regression.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cCzhgHTAp55g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing using evaluation Metric Score chart\n",
        "train_accuracy = ridge.score(X_train1,Y_train)\n",
        "test_accuracy = r2_score(Y_test,y_pred_ridge)\n",
        "print( 'Train accuracy is ',train_accuracy)\n",
        "print(' Test accuracy is ', test_accuracy)\n",
        "print(\"MSE value :\", round(mean_squared_error(Y_test, y_pred_ridge), 4))\n",
        "print(\"RMSE value :\", round(math.sqrt(mean_squared_error(Y_test, y_pred_ridge)), 4))\n",
        "print(\"MAE value :\", round(mean_absolute_error(Y_test, y_pred_ridge), 4))\n",
        "print(\"MAPE value :\", round(mean_absolute_percentage_error(Y_test, y_pred_ridge), 4))"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ridge Regression plot- Actual price vs predicted price.\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot((y_pred_ridge),color='darkorange')\n",
        "plt.plot((Y_test))\n",
        "plt.suptitle('Actual Vs Predicted Close Price:Ridge Regression', fontsize =14)\n",
        "plt.legend([\"Predicted\",\"Actual\"], fontsize=14)\n",
        "plt.xlabel('No of Test Data', fontsize= 14)\n",
        "plt.ylabel('Closing Price', fontsize= 14)\n",
        "plt.grid(True)"
      ],
      "metadata": {
        "id": "Jc4uMcpPvzlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The accuracy of model for training and testing is 95% and 94% respectively.**"
      ],
      "metadata": {
        "id": "sqnl9n3Uv6vW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-validation with hyperparameter tuning refers to the process of combining cross-validation with the search for the optimal hyperparameters of a machine learning model. The advantage of using cross-validation with hyperparameter tuning is that it provides a more reliable estimate of the model's performance by evaluating different hyperparameter combinations on multiple folds of the data. It helps to avoid overfitting and ensures that the selected hyperparameters generalize well to unseen data."
      ],
      "metadata": {
        "id": "sRz699imwaur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV)\n",
        "ridge_cv=Ridge()\n",
        "# Fit the Algorithm\n",
        "parameters = {'alpha': [1e-15,1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1e-1,1,5,10,20,30,40,45,50,55,60,100]}\n",
        "ridge_regressor = GridSearchCV(ridge_cv, parameters, scoring='neg_mean_squared_error', cv=3)\n",
        "ridge_regressor.fit(X_train1, Y_train)\n",
        "print(\"The best fit alpha value is found out to be :\" ,ridge_regressor.best_params_)\n",
        "print(\"\\nUsing \",ridge_regressor.best_params_, \" the negative mean squared error is: \", ridge_regressor.best_score_)\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ridge_best= Ridge(alpha=0.01)\n",
        "ridge_best.fit(X_train1,Y_train)\n"
      ],
      "metadata": {
        "id": "rg5dFW8e0BWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_rcv= ridge_best.predict(X_test1)"
      ],
      "metadata": {
        "id": "4ze11Xsf0CBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing using evaluation Metric Score chart\n",
        "train_accuracy = ridge_best.score(X_train1,Y_train)\n",
        "test_accuracy = r2_score(Y_test,y_pred_rcv)\n",
        "print( 'Train accuracy is ',train_accuracy)\n",
        "print(' Test accuracy is ', test_accuracy)\n",
        "print(\"MSE value :\", round(mean_squared_error(Y_test, y_pred_rcv), 4))\n",
        "print(\"RMSE value :\", round(math.sqrt(mean_squared_error(Y_test, y_pred_rcv)), 4))\n",
        "print(\"MAE value :\", round(mean_absolute_error(Y_test, y_pred_rcv), 4))\n",
        "print(\"MAPE value :\", round(mean_absolute_percentage_error(Y_test, y_pred_rcv), 4))"
      ],
      "metadata": {
        "id": "IoQ-H42e0NLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ridge Regression plot after cross-validation - Actual price vs predicted price.\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot((y_pred_rcv),color='black')\n",
        "plt.plot((Y_test))\n",
        "plt.suptitle('Actual Vs Predicted Close Price:Ridge Regression(cross validation)', fontsize =14)\n",
        "plt.legend([\"Predicted\",\"Actual\"], fontsize=14)\n",
        "plt.xlabel('No of Test Data', fontsize= 14)\n",
        "plt.ylabel('Closing Price', fontsize= 14)\n",
        "plt.grid(True)"
      ],
      "metadata": {
        "id": "5dXsWrpo0PFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used GridSearch cross validation technique. It gives functionality for parameter evaluating at the given regularizrion technique."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* In Ridge regularization the training and testing accuracy is 95% and 94% respectively with Rmse 0.2188\n",
        "* After cross validation the ridge accuracy for training increased to 97% but testing accuracy remains 94% with Rmse 0.2212\n",
        "\n",
        "After applying cross validation, the testing accuracy remains same as in L2 regularizarion but training accuracy increased to 97% which shows a  slightly higher risk of overfitting compared to the regularized Ridge model ,so preferably one must go with regularized Ridge model as it offers good performance, while also promoting model simplicity and interpretability.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 4"
      ],
      "metadata": {
        "id": "mP3oMIWm1RoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 4 Implementation\n",
        "elastic_reg = ElasticNet(alpha=0.001,l1_ratio=0.005)\n",
        "# Fit the model\n",
        "elastic_reg.fit(X_train1,Y_train)\n",
        "#Predict the model\n",
        "y_pred_elastic= elastic_reg.predict(X_test1)"
      ],
      "metadata": {
        "id": "fEhJksgB1fWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "rzH7ww_51mIp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Elastic Net** is a regression method that performs variable selection and regularization both simultaneously. The term regularization is the main concept behind the elastic net. Regularization comes into picture when the model is overfitted. Now we need to understand what overfitting means, so overfitting is a problem that occurs when the model is performing good with the training dataset, but with the test, dataset model is giving errors; in this situation the regularization is a technique to reduce the errors by fitting a function appropriately in the training dataset. These functions can be called penalties."
      ],
      "metadata": {
        "id": "uBjMa9hZ1rhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing using evaluation Metric Score chart\n",
        "train_accuracy = elastic_reg.score(X_train1,Y_train)\n",
        "test_accuracy = r2_score(Y_test,y_pred_elastic)\n",
        "print( 'Train accuracy is ',train_accuracy)\n",
        "print(' Test accuracy is ', test_accuracy)\n",
        "print(\"MSE value :\", round(mean_squared_error(Y_test, y_pred_elastic), 4))\n",
        "print(\"RMSE value :\", round(math.sqrt(mean_squared_error(Y_test, y_pred_elastic)), 4))\n",
        "print(\"MAE value :\", round(mean_absolute_error(Y_test, y_pred_elastic), 4))\n",
        "print(\"MAPE value :\", round(mean_absolute_percentage_error(Y_test, y_pred_elastic), 4))"
      ],
      "metadata": {
        "id": "d6S8TZSR1xUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Elastic Net plot- Actual price vs predicted price.\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot((y_pred_elastic),color='darkred')\n",
        "plt.plot((Y_test))\n",
        "plt.suptitle('Actual Vs Predicted Close Price:Elastic Net', fontsize =14)\n",
        "plt.legend([\"Predicted\",\"Actual\"], fontsize=14)\n",
        "plt.xlabel('No of Test Data', fontsize= 14)\n",
        "plt.ylabel('Closing Price', fontsize= 14)\n",
        "plt.grid(True)"
      ],
      "metadata": {
        "id": "pCSycPz611iK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning of Elastic Net"
      ],
      "metadata": {
        "id": "5S5aBkJ019Ep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-validation with hyperparameter tuning refers to the process of combining cross-validation with the search for the optimal hyperparameters of a machine learning model. The advantage of using cross-validation with hyperparameter tuning is that it provides a more reliable estimate of the model's performance by evaluating different hyperparameter combinations on multiple folds of the data. It helps to avoid overfitting and ensures that the selected hyperparameters generalize well to unseen data."
      ],
      "metadata": {
        "id": "X3eZJkkG2BSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 4 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV)\n",
        "elastic = ElasticNet()\n",
        "# Fit the Algorithm\n",
        "parameters = {'alpha':[1e-15,1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1e-1,1,5,10,20,30,40,45,50,55,60,100],'l1_ratio':[0.3,0.4,0.5,0.6,0.7,0.8,1,2]}\n",
        "elastic_model= GridSearchCV(elastic,parameters,scoring='neg_mean_squared_error',cv=3)\n",
        "elastic_model.fit(X_train1,Y_train)\n",
        "print(\"The best fit alpha and L1 ratio value is found out to be :\" ,elastic_model.best_params_['alpha'], elastic_model.best_params_['l1_ratio'])\n",
        "print(\"The negative mean squared error for is: \", round(elastic_model.best_score_,3))"
      ],
      "metadata": {
        "id": "BB04R1Fl2JXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "elastic_best= ElasticNet(alpha=0.001,l1_ratio=1)\n",
        "elastic_best.fit(X_train1,Y_train)"
      ],
      "metadata": {
        "id": "kYsrMk-b2JvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Predict the model\n",
        "y_pred_elastic_cv= elastic_best.predict(X_test1)"
      ],
      "metadata": {
        "id": "euxE_Ty92KOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Visualizing using evaluation Metric Score chart\n",
        "train_accuracy = elastic_best.score(X_train1,Y_train)\n",
        "test_accuracy = r2_score(Y_test,y_pred_elastic_cv)\n",
        "print( 'Train accuracy is ',train_accuracy)\n",
        "print(' Test accuracy is ', test_accuracy)\n",
        "print(\"MSE value :\", round(mean_squared_error(Y_test, y_pred_elastic_cv), 4))\n",
        "print(\"RMSE value :\", round(math.sqrt(mean_squared_error(Y_test, y_pred_elastic_cv)), 4))\n",
        "print(\"MAE value :\", round(mean_absolute_error(Y_test, y_pred_elastic_cv), 4))\n",
        "print(\"MAPE value :\", round(mean_absolute_percentage_error(Y_test, y_pred_elastic_cv), 4))"
      ],
      "metadata": {
        "id": "qD3kdHfr2KpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Elastic Net plot after cross validation- Actual price vs predicted price.\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot((y_pred_elastic_cv),color='magenta')\n",
        "plt.plot((Y_test))\n",
        "plt.suptitle('Actual Vs Predicted Close Price:Elastic Net(Cross Validation)', fontsize =14)\n",
        "plt.legend([\"Predicted\",\"Actual\"], fontsize=14)\n",
        "plt.xlabel('No of Test Data', fontsize= 14)\n",
        "plt.ylabel('Closing Price', fontsize= 14)\n",
        "plt.grid(True)"
      ],
      "metadata": {
        "id": "rsVBiMSg2K9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PwkmIYw22jpy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used GridSearch cross validation technique. It gives functionality for parameter evaluating at the given regularizrion technique."
      ],
      "metadata": {
        "id": "GbVlC0xL2kJz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "q4MIDqFK2kf2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* In Elastic Net the training and testing accuracy is 96% and 94% respectively with Rmse 0.2225\n",
        "* After Cross-Validation the Elastic Net accuracy for training is 96% which remains the same but the testing acuuracy decreases to 93% with Rmse 0.2297\n",
        "\n"
      ],
      "metadata": {
        "id": "xtr98gnH2k3-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 5"
      ],
      "metadata": {
        "id": "zH3KWKGY23bj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 4 Implementation\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "RFR = RandomForestRegressor()\n",
        "# Fit the Algorithm\n",
        "RFR.fit(X_train1, Y_train)\n",
        "# Predict on the model\n",
        "y_pred_RFR= RFR.predict(X_test1)"
      ],
      "metadata": {
        "id": "RTp0QhrJ24H0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "nMxH9blB24kj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Random Forest is a popular machine learning algorithm used for both classification and regression tasks. It belongs to the ensemble learning family, which combines multiple individual models to create a more powerful and robust predictive model.\n",
        "\n",
        "The basic idea behind the Random Forest algorithm is to build a \"forest\" of decision trees and make predictions by aggregating the results of each individual tree. Each decision tree in the forest is constructed using a random subset of the training data and a random subset of the features (input variables). This randomness helps to introduce diversity among the trees and prevents overfitting."
      ],
      "metadata": {
        "id": "PLKhnbaM3Dkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing using evaluation Metric Score chart\n",
        "train_accuracy = RFR.score(X_train1, Y_train)\n",
        "test_accuracy = r2_score(Y_test, y_pred_RFR)\n",
        "print('Train accuracy is ',train_accuracy)\n",
        "print('Test accuracy is ', test_accuracy)\n",
        "print(\"MSE value :\", round(mean_squared_error(Y_test, y_pred_RFR), 4))\n",
        "print(\"RMSE value :\", round(math.sqrt(mean_squared_error(Y_test, y_pred_RFR)), 4))\n",
        "print(\"MAE value :\", round(mean_absolute_error(Y_test, y_pred_RFR), 4))\n",
        "print(\"MAPE value :\", round(mean_absolute_percentage_error(Y_test, y_pred_RFR), 4))"
      ],
      "metadata": {
        "id": "YPiG7V0-3Knt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest Regressor plot - Actual price vs predicted price.\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot((y_pred_RFR),color='pink')\n",
        "plt.plot((Y_test))\n",
        "plt.suptitle('Actual Vs Predicted Close Price:Random Forest', fontsize =14)\n",
        "plt.legend([\"Predicted\",\"Actual\"], fontsize=14)\n",
        "plt.xlabel('No of Test Data', fontsize= 14)\n",
        "plt.ylabel('Closing Price', fontsize= 14)\n",
        "plt.grid(True)"
      ],
      "metadata": {
        "id": "pWt-zirK3LFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning of Random Forest"
      ],
      "metadata": {
        "id": "WxX2ix-93Vtj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-validation with hyperparameter tuning refers to the process of combining cross-validation with the search for the optimal hyperparameters of a machine learning model. The advantage of using cross-validation with hyperparameter tuning is that it provides a more reliable estimate of the model's performance by evaluating different hyperparameter combinations on multiple folds of the data. It helps to avoid overfitting and ensures that the selected hyperparameters generalize well to unseen data."
      ],
      "metadata": {
        "id": "zFrq0_R93WMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 5 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV)\n",
        "RFR_cv = RandomForestRegressor()\n",
        "\n",
        "# Fit the object to train dataset\n",
        "grid_values = {'n_estimators':[50, 80,  100], 'max_depth':[3, 5, 7]}\n",
        "RFR_cv_grid = GridSearchCV(RFR_cv, param_grid = grid_values, scoring = 'neg_mean_squared_error', cv=5)\n",
        "\n",
        "# Fit the Algorithm\n",
        "RFR_cv_grid.fit(X_train1, Y_train)"
      ],
      "metadata": {
        "id": "l1FT3vXc3Wsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RFR_cv_grid.best_params_"
      ],
      "metadata": {
        "id": "URCm4GE53W_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RFR_best=RandomForestRegressor(max_depth=3,n_estimators=50)\n",
        "RFR_best.fit(X_train1,Y_train)"
      ],
      "metadata": {
        "id": "wsIcSWTG3XR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the model\n",
        "y_pred_RFR_CV= RFR_best.predict(X_test1)"
      ],
      "metadata": {
        "id": "jFwDk-la3XvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing using evaluation Metric Score chart\n",
        "train_accuracy = RFR_best.score(X_train1, Y_train)\n",
        "test_accuracy = r2_score(Y_test, y_pred_RFR_CV)\n",
        "print('Train accuracy is ',train_accuracy)\n",
        "print('Test accuracy is ', test_accuracy)\n",
        "print(\"MSE value :\", round(mean_squared_error(Y_test, y_pred_RFR_CV), 4))\n",
        "print(\"RMSE value :\", round(math.sqrt(mean_squared_error(Y_test, y_pred_RFR_CV)), 4))\n",
        "print(\"MAE value :\", round(mean_absolute_error(Y_test, y_pred_RFR_CV), 4))\n",
        "print(\"MAPE value :\", round(mean_absolute_percentage_error(Y_test, y_pred_RFR_CV), 4))"
      ],
      "metadata": {
        "id": "rfcYiJIm3u7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest Regressor plot after cross-validation - Actual price vs predicted price.\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot((y_pred_RFR_CV),color='peru')\n",
        "plt.plot((Y_test))\n",
        "plt.suptitle('Actual Vs Predicted Close Price:Random Forest(cross validation)', fontsize =14)\n",
        "plt.legend([\"Predicted\",\"Actual\"], fontsize=14)\n",
        "plt.xlabel('No of Test Data', fontsize= 14)\n",
        "plt.ylabel('Closing Price', fontsize= 14)\n",
        "plt.grid(True)"
      ],
      "metadata": {
        "id": "44ECXC3U329Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "fdLGSJAI33zT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used GridSearch cross validation technique. It gives functionality for parameter evaluating at the given regularizrion technique."
      ],
      "metadata": {
        "id": "9x-bv9RO34I1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "0SAzrpmu34jW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* In Random Forest the training and testing accuracy comes out to be 99% and 91% respectively with Rmse 0.2652\n",
        "* After cross validation of Random Forest the training accuracy decreases to 97% and testing accuracy remains same with Rmse 0.2694 that means this model is less prone to overfitting and it may generalize better to unseen data.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EYNbO5wc345_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 6"
      ],
      "metadata": {
        "id": "-qw196KR35QC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model- 6 - Implementation\n",
        "from xgboost import XGBRegressor\n",
        "xgb = XGBRegressor()\n",
        "# Fit the model\n",
        "xgb.fit(X_train1,Y_train)\n",
        "# Predict the model\n",
        "y_pred_xgb= xgb.predict(X_test1)\n"
      ],
      "metadata": {
        "id": "x5hZ3tDy4RLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "XRNWaVa-4Rmb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "XGBoost (eXtreme Gradient Boosting) is an advanced machine learning algorithm that belongs to the gradient boosting family. It is widely used for regression and classification tasks and has gained popularity for its high performance and scalability.The key idea behind XGBoost is to iteratively build an ensemble of weak prediction models, typically decision trees, and combine their predictions to create a strong predictive model. The algorithm optimizes a specific objective function by minimizing the loss during each boosting iteration.\n",
        "\n",
        "*XGBoost is designed to be highly efficient and scalable, making it suitable for large datasets and complex problems.*"
      ],
      "metadata": {
        "id": "nYc7OOR74SBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing using evaluation Metric Score chart\n",
        "train_accuracy = xgb.score(X_train1, Y_train)\n",
        "test_accuracy = r2_score(Y_test, y_pred_xgb)\n",
        "print('Train accuracy is ',train_accuracy)\n",
        "print('Test accuracy is ', test_accuracy)\n",
        "print(\"MSE value :\", round(mean_squared_error(Y_test, y_pred_xgb), 4))\n",
        "print(\"RMSE value :\", round(math.sqrt(mean_squared_error(Y_test, y_pred_xgb)), 4))\n",
        "print(\"MAE value :\", round(mean_absolute_error(Y_test, y_pred_xgb), 4))\n",
        "print(\"MAPE value :\", round(mean_absolute_percentage_error(Y_test, y_pred_xgb), 4))"
      ],
      "metadata": {
        "id": "Ds2s7mIb5Fmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBOOST Regressor plot - Actual price vs predicted price.\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot((y_pred_xgb),color='gold')\n",
        "plt.plot((Y_test))\n",
        "plt.suptitle('Actual Vs Predicted Close Price:XG BOOST', fontsize =14)\n",
        "plt.legend([\"Predicted\",\"Actual\"], fontsize=14)\n",
        "plt.xlabel('No of Test Data', fontsize= 14)\n",
        "plt.ylabel('Closing Price', fontsize= 14)\n",
        "plt.grid(True)"
      ],
      "metadata": {
        "id": "Ux13Xre_5F8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning of XG BOOST"
      ],
      "metadata": {
        "id": "Tu5G56Mf5GUy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-validation with hyperparameter tuning refers to the process of combining cross-validation with the search for the optimal hyperparameters of a machine learning model. The advantage of using cross-validation with hyperparameter tuning is that it provides a more reliable estimate of the model's performance by evaluating different hyperparameter combinations on multiple folds of the data. It helps to avoid overfitting and ensures that the selected hyperparameters generalize well to unseen data."
      ],
      "metadata": {
        "id": "nK3P4yqO5G01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 6 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV)\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "xgb_cv = XGBRegressor()\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "grid_values = {\n",
        "    'n_estimators': [50, 80, 100],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.1, 0.01, 0.001]\n",
        "}\n",
        "\n",
        "# Create GridSearchCV object\n",
        "xgb_cv_grid = GridSearchCV(xgb_cv, param_grid=grid_values, scoring='neg_mean_squared_error', cv=5)\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "xgb_cv_grid.fit(X_train1, Y_train)\n",
        "\n",
        "# Get the best parameters and best score\n",
        "best_params = xgb_cv_grid.best_params_\n",
        "best_score = xgb_cv_grid.best_score_\n",
        "\n",
        "# Print the best parameters and best score\n",
        "print(\"Best Parameters: \", best_params)\n",
        "print(\"Best Score: \", best_score)"
      ],
      "metadata": {
        "id": "kl9gSkNe5a1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "XGB_best= XGBRegressor(max_depth=3,n_estimators=50,learning_rate=0.1)\n",
        "XGB_best.fit(X_train1,Y_train)"
      ],
      "metadata": {
        "id": "CvZjHCvc5bZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_xgb_cv= XGB_best.predict(X_test1)"
      ],
      "metadata": {
        "id": "mN-zMqLU5bua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing using evaluation Metric Score chart\n",
        "train_accuracy = xgb.score(X_train1, Y_train)\n",
        "test_accuracy = r2_score(Y_test, y_pred_xgb_cv)\n",
        "print('Train accuracy is ',train_accuracy)\n",
        "print('Test accuracy is ', test_accuracy)\n",
        "print(\"MSE value :\", round(mean_squared_error(Y_test, y_pred_xgb_cv), 4))\n",
        "print(\"RMSE value :\", round(math.sqrt(mean_squared_error(Y_test, y_pred_xgb_cv)), 4))\n",
        "print(\"MAE value :\", round(mean_absolute_error(Y_test, y_pred_xgb_cv), 4))\n",
        "print(\"MAPE value :\", round(mean_absolute_percentage_error(Y_test, y_pred_xgb_cv), 4))"
      ],
      "metadata": {
        "id": "v1k22Cuz5cFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "rNR64kK05r6s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used GridSearch cross validation technique. It gives functionality for parameter evaluating at the given regularizrion technique."
      ],
      "metadata": {
        "id": "n-iglvvM5sRq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Hs-7CPkQ5sqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* In XG BOOST model the training and testing accuracy comes out to be 99% and 91% respectively which is sign of slightly overfitting.\n",
        "* After applyting Cross Validation to XG BOOST, training and testing accuracy remains same as it was earlier in normal XG BOOST, it means  the default hyperparameters provided by XGBoost are sufficient as well as optimized, and there is no need for further hyperparameter tuning.\n",
        "\n"
      ],
      "metadata": {
        "id": "Gk88NbbY5tFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to run all model\n",
        "def mape(actual, pred):\n",
        "    actual, pred = np.array(actual), np.array(pred)\n",
        "    return np.mean(np.abs((actual - pred) / actual)) * 100\n",
        "def score_model(X_train,y_train,X_test,y_test):\n",
        "    df_columns=[]\n",
        "    df=pd.DataFrame(columns=df_columns) # Creating dataframe to store the train and test metrics for each of the models\n",
        "\n",
        "    i=0\n",
        "\n",
        "    #Reading model one by one\n",
        "    for model in models:\n",
        "        model.fit(X_train,y_train)\n",
        "\n",
        "        y_pred_train = model.predict(X_train)\n",
        "        y_pred_test = model.predict(X_test)\n",
        "\n",
        "        #Computing evaluation metrics\n",
        "        train_accuracy=model.score(X_train,y_train)\n",
        "        test_accuracy=model.score(X_test,y_test)\n",
        "\n",
        "        MAE=metrics.mean_absolute_error(y_test, y_pred_test)\n",
        "        MSE=metrics.mean_squared_error(y_test, y_pred_test)\n",
        "        RMSE=math.sqrt(MSE)\n",
        "        MAPE=mape(y_test, y_pred_test)\n",
        "        Rsquare=metrics.r2_score(y_test, y_pred_test)\n",
        "\n",
        "\n",
        "        #Inserting in dataframe\n",
        "        df.loc[i,\"Model_Name\"]=model.__class__.__name__\n",
        "        df.loc[i,\"MAE\"]=round(MAE,3)\n",
        "        df.loc[i,\"MSE\"]=round(MSE,3)\n",
        "        df.loc[i,\"RMSE\"]=round(RMSE,3)\n",
        "        df.loc[i,\"MAPE\"]=round(MAPE,3)\n",
        "        df.loc[i,\"Rsquare\"]=round(Rsquare,3)\n",
        "\n",
        "        i+=1\n",
        "\n",
        "    #Sorting values by accuracy\n",
        "    df.sort_values(by=['RMSE'],ascending=True,inplace=True)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "Vet7hc6dBz-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models=[reg,lasso,ridge,elastic_reg,RFR,xgb]"
      ],
      "metadata": {
        "id": "dluDgmjbB0T2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing performance\n",
        "report=score_model(X_train1,Y_train,X_test1,Y_test)\n",
        "report"
      ],
      "metadata": {
        "id": "tRoQehH7B09u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, from the above dataframe, that all models have Rsquare(R2_SCORE) greater than 90%, it means the proportion of variation in the dependent variable is very well explained by the independent variable (i.e. more than 90%) but on the contrary we must take care off the errors occuring in the models as well as how much a model is prone to overfitting. So,by keeping in mind all these perpective I choose RMSE as Error benchmark and the model having least RMSE is Ridge regularization attaning a traning accuracy of 95%.\n",
        "\n",
        "*R2_SCORE AND RMSE* are the evaluation metrics which have positive impact on business."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have considered **Ridge Reguralization** as my final prediction model which has testing accuracy of 94% and training accuracy of 95%, showing least *RMSE* value among all models and not prone to overfitting. It  offers good performance, while also promoting model simplicity and interpretability."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the coefficients (feature weights)\n",
        "coefficients = ridge.coef_\n",
        "\n",
        "# Create an array of feature names\n",
        "feature_names = np.array(['Open','High','Low'])\n",
        "\n",
        "# Sort the coefficients and corresponding feature names\n",
        "sorted_features = [feature for _, feature in sorted(zip(coefficients, feature_names))]\n",
        "sorted_coefficients = np.sort(coefficients)\n",
        "\n",
        "# Plotting the feature importance\n",
        "plt.figure(figsize=(12, 8))\n",
        "bar_width= 0.5\n",
        "plt.barh(sorted_features, sorted_coefficients)\n",
        "plt.title(\"Feature Importance in Ridge Regularization\", fontsize=16)\n",
        "plt.xlabel(\"Coefficient Value\", fontsize=12)\n",
        "plt.ylabel(\"Independent Features\", fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YqSw9h_ICSxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to the model choosen **Low** feature is contributing the most.Answer Here."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "import pickle"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ridge = Ridge()\n",
        "ridge.fit(X_train1, Y_train)"
      ],
      "metadata": {
        "id": "_ahUbc4PCij9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_file = 'ridge.pkl'\n",
        "\n",
        "# Save the model to the specified file path\n",
        "with open(model_file, 'wb') as file:\n",
        "    pickle.dump(ridge, file)"
      ],
      "metadata": {
        "id": "p-_rkhGJCi75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the file path of the saved model\n",
        "model_file = 'ridge.pkl'\n",
        "\n",
        "# Load the model from the pickle file\n",
        "with open(model_file, 'rb') as file:\n",
        "    loaded_model = pickle.load(file)"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict using the loaded model and x_test1\n",
        "y_pred1 = loaded_model.predict(X_test1)"
      ],
      "metadata": {
        "id": "609hN6lbCt5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred1"
      ],
      "metadata": {
        "id": "iELPHoB9CuVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_test"
      ],
      "metadata": {
        "id": "mpPbMsEyCvPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Closing Price** of a stock refers to the final price at which the stock is traded on a particular stock exchange on a given trading day. It is the last price at which the stock is bought or sold during the trading session.\n",
        "\n",
        "*Importance:* The closing price is an important metric used by investors, analysts, and traders to evaluate a companys financial health, market value, and stock performance. It is also used to calculate other important metrics such as the daily price change, market capitalization, and trading volume.\n",
        "\n",
        "For an *Average Investor:* An average investor sees investing in stocks for long-term purposes and in premium stocks that have proved to be quality and high-performing stocks over the years. For such investors, the daily closing price may not hold as high importance as for an average trader.\n",
        "\n",
        "For a *Traders:* For traders and analysts, the information on the closing price of stocks is essential to make sure that they make sound trading decisions and maximize returns on their portfolios.\n",
        "\n",
        "**Models Implemented**\n",
        "* I have implemented six models/Algorithms- Linear Regression,Lasso Reguralization,Ridge Reguralization,Elastic Net, Random Forest Regressor, XG BOSST Regressor.\n",
        "* The best performing model to predict the closing stock price of yes bank is **Ridge Reguralization** having training and testing accuracy as 95% and 94% respectively in untuned condition having lowest *RMSE*(.219), with cross-validation and hyperparameter tuning(*Tuned Model*) its training and testing accuracy is 97% and 94% respectively which tends to generalized fitting and prone to slight-risk of overfitting.\n",
        "\n",
        "\n",
        "* All *Independent Features*(High,Low,Open) are Strongly correlated with each other as well with the *depenedent Feature*(Close) also.\n",
        "* According to the best performing model Feature **Low** is of utmost importance.\n",
        "\n",
        "* Evaluation Metrics used to choose the best performing model is *r2_score* and *RMSE*.\n",
        "\n",
        " *The results indicate adopting supervised Learning Algorithms to identify stock manipulation using a labeled dataset based on a Fraud case is promising*.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}